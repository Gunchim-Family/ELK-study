# Logstash

### 요약
Logstash는 플러그인 기반의 오픈소스 데이터 처리 파이프라인 도구로, **데이터 수집**, **가공**, **전송**을 간편하게 처리할 수 있습니다. 다양한 입력(Input) 소스로부터 데이터를 수집하고, 필터(Filter)를 통해 데이터를 원하는 형태로 가공한 뒤, 출력(Output)을 통해 최종 목적지로 데이터를 전송합니다.


## 소개
Logstash는 데이터의 **수집**, **가공**, **전송**을 간편하게 구현하기 위한 플러그인 기반의 오픈소스 데이터 처리 파이프라인 도구
- 플러그인 기반의 오픈소스 데이터 처리 파이프라인 도구로 복잡한 데이터 전처리 과정을 간단한 설정으로 처리
- 데이터를 저장하기 전에 사용자가 원하는 형태로 변경할 수 있는 기능을 제공
- 장애 대응 로직이나 성능 저하 요인을 쉽게 파악할 수 있는 모니터링 API 제공
- 간단한 조정으로 성능을 튜닝할 수 있는 파라미터 제공
- 비츠, 로그 스태시, 엘라스틱 서치, 키바나 → 데이터 수집 - 변환 - 저장 - 시각화 구성가능

## 주요 특징

- **플러그인 기반**: 다양한 입력, 필터, 출력 플러그인을 통해 유연한 데이터 처리가 가능합니다.
- **다양한 데이터 처리**: JSON, XML, DB 데이터 마이그레이션, Elasticsearch 도큐먼트 인덱싱 등 모든 형태의 데이터를 처리할 수 있습니다.
- **고성능 및 안정성**: 내장된 메모리 및 파일 기반 큐를 통해 높은 처리 속도와 안정성을 제공합니다.
- **장애 대응**: Retry 로직 및 dead letter queue를 통해 장애 상황에서의 데이터 유실을 최소화합니다.

## 기본 사용 방법

Logstash의 데이터 처리 파이프라인은 **입력(Input)**, **필터(Filter)**, **출력(Output)**의 세 가지 주요 단계로 구성됩니다.

### 1. 입력 (Input)

파이프라인의 앞부분에 위치하며, 소스 원본으로부터 데이터를 입력받는 역할을 합니다.

자주 사용하는 입력 플러그인:
- **file**: 파일을 스트리밍하며 이벤트를 읽어들임.
- **syslog**: 네트워크를 통해 전달되는 시스템 로그를 수신.
- **kafka**: 카프카 토픽에서 데이터를 읽어 들임.
- **jdbc**: JDBC 드라이버를 사용하여 일정 간격으로 쿼리를 실행해 결과를 읽어 들임.

### 2. 필터 (Filter)

입력 플러그인이 받은 데이터를 의미 있는 데이터로 구조화하는 역할을 하며, 비정형 데이터를 정형화하고 데이터 분석을 위한 구조를 만듭니다.

자주 사용하는 필터 플러그인:
- **grok**: 정규식과 유사한 grok 패턴을 사용해 메시지를 구조화된 형태로 분석.
- **dissect**: 간단한 패턴을 사용해 메시지를 구조화된 형태로 분석 (grok보다 빠름).
- **mutate**: 필드명 변경, 문자열 처리 등 일반적인 가공 함수 제공.
- **date**: 문자열을 지정한 패턴의 날짜 형식으로 분석.

### 3. 출력 (Output)

필터를 거쳐 가공된 데이터를 지정한 대상으로 내보내는 단계입니다.

자주 사용하는 출력 플러그인:
- **elasticsearch**: 가장 많이 사용되는 출력 플러그인으로, 데이터를 Elasticsearch로 전송.
- **file**: 지정한 파일의 새로운 줄에 데이터를 기록.
- **kafka**: 카프카 토픽에 데이터를 publish.

### 4. 코덱 (Codec)

입력과 출력 시 메시지를 적절한 형태로 변환하는 스트림 필터 플러그인입니다.

- **json**: 입력 시 JSON 형태의 메시지를 객체로 읽어들이고, 출력 시 다시 JSON 형태로 변환.
- **plain**: 단순 문자열로 메시지를 처리하며, 출력 시 원하는 포맷으로 지정 가능.
- **rubydebug**: 로그스태시 설정을 테스트하거나 파이프라인 설정 오류를 디버깅하는 데 사용. 출력 시 루비 언어의 해시 형태로 이벤트를 기록.

## 다중 파이프라인

Logstash는 하나의 인스턴스에서 여러 개의 파이프라인을 동시에 실행할 수 있습니다. 예를 들어:

- **파이프라인 1**: beat → dissect → elasticsearch
- **파이프라인 2**: kafka → grok → elasticsearch

이를 통해 다양한 소스와 목적지에 대해 효율적으로 데이터를 처리할 수 있습니다.

## 설정

Logstash의 파이프라인 설정은 `pipelines.yml` 파일에서 수행됩니다. 이 파일을 수정하여 각 파이프라인의 입력, 필터, 출력 플러그인을 정의할 수 있습니다.

## 모니터링

Logstash는 다양한 모니터링 API를 제공하여 성능 및 상태를 실시간으로 확인할 수 있습니다.

- **노드 정보**: `ip:port/_node?pretty`
- **플러그인 정보**: `ip:port/_node/plugins?pretty`
- **노드 통계**: `ip:port/_node/stats?pretty`
- **핫 스레드**: `ip:port/_node/hot_threads?pretty`

### 타입별 모니터링 정보

- **파이프라인**: 실행 중인 파이프라인의 종류와 할당된 배치 크기, 워커 수 정보를 제공합니다.
- **OS**: 운영체제의 종류 및 버전 정보를 확인할 수 있습니다.
- **JVM**: 노드의 JVM 버전, GC 방식, 메모리 정보 등을 제공합니다.
